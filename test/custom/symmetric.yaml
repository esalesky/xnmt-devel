symmetric: !Experiment
  exp_global: !ExpGlobal
    model_file: '{EXP_DIR}/models/{EXP}.mod'
    log_file: '{EXP_DIR}/logs/{EXP}.log'
    default_layer_dim: 512
    dropout: 0.3
    compute_report: True
  model: !SymmetricTranslator
    mode: teacher
    sampling_prob: !DefinedSequence { initial: 1.0 }
#    unfold_until: supervised
    src_reader: !CompoundReader
      vocab: !Vocab {vocab_file: examples/data/head.ja.vocab}
      readers:
      - !PlainTextReader
        vocab: !Vocab {vocab_file: examples/data/head.ja.vocab}
      - !PlainTextReader
        vocab: !Vocab {vocab_file: examples/data/head.ja.vocab}
    trg_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: examples/data/head.en.vocab}
    src_embedder: !SimpleWordEmbedder
      emb_dim: 512
      vocab: !Ref { path: model.src_reader.readers.0.vocab }
    trg_embedder: !DenseWordEmbedder
      vocab: !Ref { path: model.trg_reader.vocab }
    encoder: !BiLSTMSeqTransducer
      layers: 1
    attender: !MlpAttender
      hidden_dim: 512
      state_dim: 512
      input_dim: 512
    dec_lstm: !UniLSTMSeqTransducer
      layers: 1
    transform: !AuxNonLinear {}
    scorer: !Softmax {}
    bridge: !CopyBridge {}
  train: !SimpleTrainingRegimen
    batcher: !InOrderBatcher
      batch_size: 10
    trainer: !AdamTrainer
      alpha: 0.001
    run_for_epochs: 1
    src_file: [examples/data/head.ja, examples/data/head.en]
    trg_file: examples/data/head.en
    dev_tasks:
      - !LossEvalTask
        src_file: examples/data/head.ja
        ref_file: examples/data/head.en
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: [examples/data/head.ja, examples/data/head.en]
      ref_file: examples/data/head.en
      hyp_file: examples/output/{EXP}.test_hyp
      inference: !IndependentOutputInference
        post_process: join-char
        mode: forceddebug
        ref_file: examples/data/head.en
        batcher: !InOrderBatcher
          batch_size: 1
        reporter: !AttentionReporter {}
